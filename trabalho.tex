% =======================================================================
% =                                                                     =
% = ABNTEX - UTP                                                        =
% =                                                                     =
% =======================================================================
% -----------------------------------------------------------------------
% Author: Chaua Queirolo
% Data:   01/07/2017
% -----------------------------------------------------------------------
\documentclass[12pt,oneside,a4paper,chapter=TITLE,section=TITLE,sumario=tradicional]{abntex2}

% Regras da abnt
\usepackage{abnt-UTP}
\usepackage{lipsum}

% =======================================================================
% =                                                                     =
% = DADOS DO TRABALHO                                                   =
% =                                                                     =
% =======================================================================

% Informações de dados para CAPA e FOLHA DE ROSTO
\titulo{Segmentação Automática de Pessoas em Vídeo utilizando Raspberry Pi}

\autor{Matheus Mendes da Silva Santos}

\orientador{Prof. MSc. Chauã Coluene Queirolo Barbosa da Silva}

\preambulo{Trabalho de Conclusão de Curso apresentado ao curso de Bacharelado 
em Ciência da Computação da Faculdade de Ciências Exatas e de Tecnologia da 
Universidade Tuiuti do Paraná, como requisito à obtenção ao grau de Bacharel.}

\instituicao{Universidade Tuiuti do Paraná}
\local{Curitiba}
\data{2017}

% =======================================================================
% =                                                                     =
% = DOCUMENTO                                                           =
% =                                                                     =
% =======================================================================
\begin{document}

% -----------------------------------------------------------------------
% -                                                                     -
% - ELEMENTOS PRÉ-TEXTUAIS                                              -
% -                                                                     -
% -----------------------------------------------------------------------

% Capa e folha de rosto
\imprimircapa
\imprimirfolhaderosto

% Resumo
\begin{resumo}
Com o surgimento e a viabilização econômica de sistemas e projetos embarcados, o ensino da computação se popularizou no meio educacional, fazendo o número de pesquisadores e usuários crescer rapidamente. Por isso a área de miniaturização de computadores e adoção dos sistemas embarcados vem colocando-se no meio profissional de maneira gradativa. O presente trabalho trata dos conceitos de Processamento Digital de Imagens aplicados ao Raspberry Pi, módulo de computação embarcada, e sua câmera. Utilizando técnicas de segmentação no vídeo adquirido, foi desenvolvido um método de segmentação de pessoas em tempo real.  Este projeto pode ser aplicado em sistemas de vigilância, segurança e rastreamento. Os conhecimentos adquiridos com o desenvolvimento deste projeto somam funcionalidade e prática aos conceitos científicos e técnicos dos estudos de Ciência da Computação.

\palavraschave{Sistemas Embarcados, Raspberry Pi, Processamento Digital de Imagens, Segmentação de Pessoas, Vídeo}    
\end{resumo}

% Listas
\listadefiguras
%%\listadegraficos
%%\listadetabelas
\listadequadros
%%\listadecodigos
%%\listadealgoritmos

% Lista de siglas
\begin{siglas}
  \item[PDI] Processamento Digital de Imagens
\end{siglas}
% ---

% Lista de símbolos
%%\begin{simbolos}

%%\end{simbolos}

% Sumario
\sumario

% -----------------------------------------------------------------------
% -                                                                     -
% - ELEMENTOS TEXTUAIS                                                  -
% -                                                                     -
% -----------------------------------------------------------------------
% Inicia a numeracao das páginas
\textual

% -----------------------------------------------------------------------
% -----------------------------------------------------------------------
\chapter{Introdução}
\label{cap:introducao}

Diversos projetos de software embarcado têm surgido nos últimos anos, motivados principalmente pela evolução dos chips, processadores e aumento da capa-cidade computacional. Em 1965, Gordon Moore\footnote{\url{ http://www.monolithic3d.com/uploads/6/0/5/5/6055488/gordon_moore_1965_article.pdf	
}}  já havia feito a previsão de que cada vez mais haveriam componentes menores e com maior processamento.
Dois componentes populares são o Arduino e Raspberry Pi.  Essas placas, do tamanho de cartões de crédito, possuem o mesmo poder computacional de estações de trabalho convencionais, além de um custo muito menor.   O estudo de Processamento Digital de Imagens (PDI) tem sido empregado em campos da área industrial, saúde, segurança e robótica. Esse último, mais recentemente tem ganhado maior vínculo com a área de PDI, pois tem buscado a automação e a execução de tarefas mais complexas com o auxílio de sensores.

O objetivo central deste trabalho é o desenvolvimento de uma aplicação embarcada para a plataforma \textit{Raspberry Pi}, tendo como funcionalidades:  

\begin{enumerate}
    \item Processamento em tempo real dos quadros do vídeo capturado a partir do módulo de câmera;
    \item Realização das etapas de segmentação de pessoas dos quadros do vídeo.
\end{enumerate}

Este trabalho está dividido em:
\begin{lista}
    \item O Capítulo 2 apresenta a funda-mentação teórica abordando os conceitos de PDI, processamento embarcado no Raspberry Pi e o problema de segmentação de pessoas em vídeo.
    \item O Capítulo 3 apresenta os trabalhos relacionados com o tema proposto neste trabalho.
    \item Seguido da metodologia de desenvolvimento do sistema proposto no Capítulo 4.
\end{lista}


% -----------------------------------------------------------------------
% -----------------------------------------------------------------------
\chapter{Fundamentação Teórica}
\label{cap:fundamentacao-teorica}

Este capítulo apresenta os fundamentos necessários para a implementação deste projeto. Inicialmente são apresentados os conceitos relacionados ao PDI. Em seguida são apresentados detalhes sobre o Raspberry Pi e, finalmente, são descritas as características do problema de segmentação de pessoas em vídeo.


\section{PROCESSAMENTO DIGITAL DE IMAGENS}

O conceito de processamento de imagens refere-se ao conjunto de métodos utilizados em imagens adquiridas, que devem ser corrigidas e melhor definidas, isso para que a interpretação visual se torne mais nítida e pontos perdidos na aquisição, e também na transmissão, possam ser restaurados com a finalidade de se obter a qualidade mais próxima possível da vida real.

\subsection{Fundamentos das Imagens Digitais}

Uma imagem pode ser definida como uma função de intensidade de luz sobre determinado ponto, tratada comumente de maneira bidimensional. (GONZALEZ, e WOODS, 2006, p.4). Explorando de maneira minuciosa a definição de imagem, tem-se que para a obtenção de uma imagem, em um plano de coordenadas espaciais, dado por $(x, y)$ deve-se estabelecer um ponto de observação, também denominado ponto focal, como conforme mostra a \autoref{fig:aquisicao_sensor_felgueiras}, medir os graus de incidência e reflexão da luz gerada e presente no plano observável em consideração ao ponto de observação e sua redondeza, denominada cena.

A incidência ou iluminação de pontos na cena é obtida partindo da fonte de luz para os elementos presentes no plano. A resultante varia de acordo com a exposição e a luminosidade do instante no qual a imagem é capturada. Já a reflexão, ou refletância do ponto de observação, corresponde ao grau de absorção que determinado objeto tem da quantidade de luz emitida no ambiente. Tipicamente, o valor é definido através de um intervalo entre 0 e 1, onde 1 é a reflexão total do comprimento de onda referente à luz incidida sobre o objeto.


\begin{figure}[htb]
    \legenda[fig:aquisicao_sensor_felgueiras]{Aquisição por sensor}
    \fig{scale=0.5}{img/aquisicao_sensor_felgueiras}
    \fonte{\citeonline[p. 24]{felgueiras2008}}
\end{figure}

Em resumo, uma imagem é o conjunto de pontos convergentes que constrói um todo. Para o estudo e processamento, imagem é o conjunto de informações coletadas para a representação mais nítida de um ponto de observação. Os pontos focais, no mundo real, são definidos como tridimensionais, por serem traduzidos em três coordenadas, $x, y, z$, respectivamente altura, largura e profundidade. Quando há captura através de um sistema, seja com câmeras ou sensores, e transcrição para uma imagem, existe também perda da informação espacial, correspondente à coordenada $z$. Logo, a profundidade da cena na qual o objeto, ponto focal, está é omitida (\cite{pdi2006}).

\subsection{Imagens Bidimensionais}

O conceito de imagem bidimensional, corriqueiramente citado como 2D, refere-se à imagem obtida puramente através de câmeras, onde ocorre a perda da coordenada de profundidade do espaço-objeto. A imagem digital é constituída através de uma função, $f(x,y)$, descrita na Equação (1), como coordenadas espaciais transcritas por meio de uma matriz (\autoref{fig:representacao_matriz_felgueiras}), onde índices referem-se à posição de um pixel na imagem.

$f(x,y)$ = \begin{matriz}\label{m2}
\left( {\begin{array}{*{20}c}{$f(0,0)$ & $f(0,1)$ & \cdots & $f(0, n-1)$ \\ $f(1,0)$ & $f(1,1)$ & \cdots & $f(1, n-1)$ \\ \vdots & \vdots & \vdots & \\ $f(m-1,0)$ & $f(m-1, 1)$ & \cdots & $f(m-1, n-1)$} \end{array} } \right)
\end{matriz}

\begin{figure}[htb]
    \legenda[fig:representacao_matriz_felgueiras]{Representação matricial de uma imagem}
    \fig{scale=0.75}{img/representacao_matriz_felgueiras}
    \fonte{\citeonline[p. 41]{felgueiras2008}}
\end{figure}

A cor ou intensidade de uma imagem é obtida através do valor de cada tupla existente na matriz em escala de cinza (medida que varia de 0 a 255), como apresentado na \autoref{fig:digitalizacao_imagem_felgueiras}. Para a aquisição de uma imagem bidimensional é necessário um equipamento capaz de identificar níveis de onda eletromagnética (raio-X, infravermelho, ultravioleta, entre outros) e produzir um sinal elétrico correspondente para que um segundo equipamento possa transcrever o sinal físico para o meio digital.

\begin{figure}[htb]
    \legenda[fig:digitalizacao_imagem_felgueiras]{Digitalização de uma imagem}
    \fig{scale=1.0}{img/digitalizacao_imagem_felgueiras}
    \fonte{\citeonline[p. 39]{felgueiras2008}}
\end{figure}

Os sinais identificados e transcritos passam pela aplicação de modelos e técnicas de processamento para eliminar ou sobrepor o eixo $z$, eixo da profundidade. Chamada de transformação de perspectiva, ou imageamento, essa é uma técnica que busca a aproximação da imagem identificada tridimensionalmente pelo olho.
Após a transcrição do sinal recebido, é necessário armazenar a matriz para que, posteriormente, aplique-se à mesma o processamento.

\newline Esse armazenamento pode ocorrer por curto prazo, no qual as ondas são armazenadas em buffers para que, dependendo do dispositivo, possa ocorrer nele próprio, por exemplo, um processamento inicial, sendo apenas a tradução para pixels ou até a montagem de uma imagem digital. Outro tipo de armazenamento típico é o massivo, onde os dados coletados são retidos para que, posteriormente, sejam analisados por programas para construir as imagens (\citeonline{pdi2006}).

\subsection{Vídeo}
Vídeo digital é o resultado de uma sequência de quadros de imagens digitais \textit{(frames)}, representando uma determinada cena. A taxa de atualização de cada frame se dá pela razão de quadros de imagens digitais por tempo contínuo, contado em segundos, e sua exibição é dada progressivamente.
O armazenamento de vídeos leva em consideração a qualidade e o tamanho. Para isso, vários \textit{codecs} (formatos) foram criados e são utilizados conforme a necessidade de recuperação das informações do vídeo e o suporte do hardware onde será reproduzido \cite[p. 6]{de2013arquitetura}.

\subsection{Passos Fundamentais do PDI}
Nesta seção são apresentados e definidos os passos fundamentais do Processamento Digital de Imagens (PDI), mostados na \autoref{fig:passos_pdi_gonzalez}.

\begin{figure}[htb]
    \legenda[fig:passos_pdi_gonzalez]{Passos fundamentais do Processamento Digital de Imagens}
    \fig{scale=0.5}{img/passos_pdi_gonzalez}
    \fonte{\citeonline{pdi2006}}
\end{figure}

\subsection{Aquisição de Imagens}

A primeira etapa do PDI é definida por traduzir os aspectos visíveis do mundo real por meio de sensores capazes capturar uma banda eletromagnética e traduzir em sinais elétricos perceptíveis, bem como converter tais sinais em elementos digitais. Como exemplos de sensores, tem-se satélites, aparelhos de ressonância magnética e câmeras fotográficas (\citeonline[p.7 – 10]{pdi2006}).

\subsection{Pré-Pocessamento}

A etapa de pré-processamento é responsável pela melhoria da imagem para aumento de qualidade, de modo algorítmico, para os processos seguintes. O pré-processamento consiste na aplicação das técnicas de melhoria de contraste, realce de características e correção de defeitos de captura, como por exemplo ruídos e foco (\citeonline[p.6]{pdi2006}).

\subsection{Segmentação}

Segundo \citeonline[p.6]{pdi2006}, a etapa de segmentação é responsável por separar a imagem em partes, regiões ou objetos de interesse e destacá-los em primeiro plano. Para a segmentação a imagem é tratada através dos valores reais dos \textit{pixels} existentes, ou da sua presença ou ausência (modo binário). Tal processo baseia-se em três propriedades de uma imagem (\citeonline[p. 236 e 237]{solomon2000fundamentos}):
\begin{lista}
    \item Cor: Diferença do espaço de cores para definir uma região;
    \item Textura: Diferenciação estatística de intensidade espacial em uma imagem;
    \item Movimento: Subtração de quadros de imagens em fundo estacionário que pode definir com precisão a movimentação de um objeto.
\end{lista}

\subsection{Representação e Descrição}

Conforme \citeonline[p.6]{pdi2006}, a representação é a etapa de definição e separação adequada dos objetos segmentados conforme a necessidade da análise posterior, reconhecimento e interpretação. Já a descrição é a etapa quantitativa das características dos objetos segmentados e representados, com a finalidade de resultar em informações de interesse ou classificação.

\subsection{Reconhecimento e Interpretação}

A etapa de reconhecimento e interpretação, segundo \citeonline[p.7 – 10]{pdi2006}, é a rotulação baseada na descrição de cada objeto presente na imagem e a atribuição de um significado, conforme a base de conhecimento, aos resultados adquiridos.

\subsection{Base de Conhecimento}
Dado como o objeto ou o conjunto de imagens a ser estudado pelo Processamento Digital de Imagens, a base de conhecimento é responsável por guiar cada etapa e definir a interação entre as mesmas \citeonline[p.6]{pdi2006}. Para que os objetos de processamento sejam acessados, o armazenamento é realizado em três categorias:
\begin{enumerate}
    \item De curto tempo: Armazenamento da imagem enquanto processada, em memória computacional (\textit{frame buffers}), onde a velocidade de acesso é alta, isto é, cerca de 30 imagens por segundo.
    \item Em massa ou \textit{on-line}: Armazenamento em unidades com capacidade para, no mínimo, algumas centenas de imagens contabilizadas em MB. Para que as informações estejam disponíveis e possam ser acessadas de maneira rápida, essas são comprimidas juntamente com suas informações (tamanho, número de cores, entre outras) em diferentes formatos.
    \item Arquivamento: Armazenamento em massa sem a necessidade corrente de acesso, onde agrupamentos de imagens são estocados.
\end{enumerate}

\section{Técnicas de Processamento de Imagens}
\subsection{Limiarização}
Aplicando-se um valor de limiar à imagem, os pixels são separados em uma imagem binária por regiões, um conjunto que excede o valor definido e outro que fica abaixo do valor. O valor do limiar é definido conforme a necessidade de segmentação, seja de objetos com relação ao fundo, um valor aproximado que separe a intensidade dos pixels ou pela minimização da variância em uma classe de pixels (chamado de método Otsu). (\citeonline[p. 237, 238]{solomon2000fundamentos}) 

\subsection{Erosão}
Eliminação de pixels não coerentes com o espaço onde estão inseridos. Aumento da lacuna entre regiões, produzindo uma imagem reduzida como um todo.

\subsection{Dilatação}
Metodologia contrária à erosão que visa a expansão de regiões, ampliando a região de borda, preenchendo lacunas, expandindo assim a imagem para melhor categorização, segmentação.

\subsection{Mediana}
Filtro onde o pixel central da máscara é substituído pela mediana, número que divide um conjunto de valores em dois grupos, denotando a posição central de uma distribuição ordenada de seus vizinhos. (\citeonline[p. 85]{solomon2000fundamentos})

\subsection{Histograma}
Gráfico de frequência relativa da ocorrência dos valores possíveis de cada pixel. Ou seja, a contagem de ocorrência de cada valor em escala de cinza em uma imagem. (\citeonline[p. 58, 59]{solomon2000fundamentos})

\subsection{Subtração de Imagens}
Primeiramente utilizada para ajustes em contraste de uma imagem, a subtração também pode ser aplicada a diferentes imagens (quadros, quando denotada cena) onde a resultante será a mudança ou movimentação dentro da cena. (\citeonline[p. 47]{solomon2000fundamentos})

\subsection{Regiões}
Método de separação de pixels em regiões baseado em grau de similaridade.

\subsection{Similaridade}
Agrupamento de pixels adjacentes em classes que satisfazem critérios como: diferença absoluta de intensidade entre pixels e seu valor médio em uma região, desvio-padrão entre as vizinhanças (\citeonline[p. 239, 240]{solomon2000fundamentos}).

\subsection{Detecção de Bordas}
Processo de determinação de fronteiras, contornos de uma região de interes-se, onde há a transição de intensidade entre os pixels. Tal processo é realizado aplicando-se filtros de diferenciação de gradiente, como Prewitt, Canny e Sobel, filtros laplaciano e gaussiano. (\citeonline[p. 242]{solomon2000fundamentos})

\subsection{Canny}
Método de detecção de bordas que satisfaz três critérios: baixa taxa de erro, minimi-zar a distância entre borda detectada e borda real, um único valor para a borda. Os passos são:
\begin{lista}
    \item Aplicar a redução de ruído;
    \item Determinar a intensidade da borda com a soma dos gradientes da imagem;
    \item Direcionar a borda e converter o direcionamento em valores aplicáveis ao pixel;
    \item Rastrear a borda detectada e reduzir os pixels não associados a mesma;
    \item Limiarizar a imagem para que os pixels relativos às bordas sejam definidos e conectados entre si. (\citeonline[p. 243-245]{solomon2000fundamentos}))
\end{lista}

\section{Arquitetura ARM}
A arquitetura ARM (\textit{Acorn/Advanced RISC Machine}), modelo computacional com conjunto reduzido de instruções (\textit{Reduced Instruction Set Computer}) desenvol-vido pela companhia britânica \textit{Acorn Computers (Cambridge)} para funcionalidades integradas de alto desempenho para aplicações embarcadas e para mercados emergentes, reduzindo a quantidade e tamanho dos transistores, o que reduz custo e consumo de energia. 
A implementação da microarquitetura ARM é dada em \textit{System-on-a-Chip (SoC)} ou \textit{Single-Board Computer} que compreende em uma única placa: microcontroladores (microprocessadores integrados a circuitos e memória), GPU, co-processadores. 
(ARM Processor Architecture).

A exemplo de um \textit{Single-Board-Computer}, temos: BeagleBone, Raspberry Pi, HiKey 960, Tinker Board.

\subsection{Raspberry Pi}
O \textit{Raspberry Pi} Pi é um computador com dimensões de um cartão de crédito (85.60 mm × 56.5 mm), com 45g, anunciado em 2011 (\autoref{fig:raspberry_autor}) sendo seu primeiro modelo lançado em fevereiro de 2012. Com a finalidade de ser o computador mais barato em circulação, estimular e expandir o ensino de computação e programação, foi idealizado por Pete Lomas no Reino Unido tendo seu valor em torno de 35 dólares e código aberto.

\begin{figure}[htb]
    \legenda[fig:raspberry_autor]{Raspberry Pi - modelo B (v.2011.12)}
    \fig{scale=1.0}{img/raspberry_autor}
    \fonte{O próprio autor, 2017}
\end{figure}

O computador é baseado na arquitetura ARM (\textit{Acorn/Advanced RISC Machine}), modelo computacional com conjunto reduzido de instruções (\textit{Reduced Instruction Set Computer}) criado para funcionalidades integradas de alto desempenho para aplicações embarcadas e para mercados emergentes.
Tal arquitetura possibilita a criação de técnicas de micro-arquitetura tendo o foco no tamanho da implementação, desempenho e baixo consumo de energia. Em sua placa, o \textit{Raspberry Pi} apresenta um chip \textit{Broadcom BCM2835} (modelo A/Zero) de 32 \textit{bits}, \textit{BCM2835} (modelo A) ou \textit{BCM2836/BCM2837} (modelo B) de 32/64 \textit{bits}, com processador ARM1176JZF-S, ARM \textit{Cortex-A7} e ARM \textit{Cortex-A53}, respectivamente, com velocidades de 700 MHz a 1.2 GHz, o GPU é a \textit{Videocore} IV de 250 MHz, com capacidade de reprodução em alta definição (1080p). A memória (SDRAM) é de 256 MB, 512 MB e 1 GB, para os modelos A, Zero e B, respectivamente. Também compõem a placa: portas USB 2.0, interface de entrada de vídeo CSI (\textit{Camera Serial Interface}), saída de vídeo digital HDMI e analógico através do componente GPIO (\textit{General-Purpose Input/Output}), o qual também possibilita a conexão de periféricos. A fonte de energia, via USB, é de 5 V.
Seu armazenamento se dá através de cartões de memória SD (modelo A/B), MicroSDHC (modelo B) e MMC (modelo Zero) com capacidade a ser definida pelo utilizador, onde serão instalados o sistema operacional e demais aplicações.
Os sistemas operacionais adotados para a Raspberry Pi variam conforme a sua finalidade. Em sua grande maioria, são sistemas baseados em \textit{Linux} e otimizados para o \textit{Raspberry} como o popular \textit{Raspbian} e o \textit{PiDora}. Recentemente, com o avanço da Internet das Coisas (\textit{IoT}), foi possível incluir o \textit{Windows} 10 como sistema operacional base

\subsection{Cãmera Raspberry Pi}
Módulo adicional ao \textit{Raspberry}, a \textit{Raspberry Pi Câmera} \autoref{fig:modulo_camera_autor}) é ligada ao socket CSI (\textit{Camera Serial Interface}), interface projetada especificamente para a conexão com a câmera. O primeiro modelo vem com um sensor \textit{OmniVision} OV5647 de 5 \textit{Mega Pixels} para imagens e suporte a vídeos em 1080p 30fps, 720p 60fps e VGA90. O segundo modelo, v2, traz um sensor Sony IMX219 de 8 \textit{Mega Pixels}, com maior desempenho em baixa-luminosidade e fidelidade a cores. A conexão com a porta CSI se dá através de um cabo do tipo \textit{ribbon} invertido, com 15 vias.

\begin{figure}[htb]
    \legenda[fig:modulo_camera_autor]{Módulo da câmera Raspberry Pi}
    \fig{scale=0.075}{img/modulo_camera_autor}
    \fonte{O próprio autor, 2017}
\end{figure}

% -----------------------------------------------------------------------
% -----------------------------------------------------------------------
\chapter{Revisão da Literatura}
\label{cap:revisao-literatura}

Este capítulo apresenta uma revisão dos trabalhos relacionados a detecção de pessoas em vídeo e Processamento Digital de Imagens, por meio de diferentes técnicas, aplicando-se também ao \textit{Raspberry Pi.}

\section{Image Processing on raspberry pi}
Shilpashree \textit{et al.} (2015) apresentam o processamento de imagens através do computador \textit{Raspberry Pi}, com seus componentes, como, por exemplo, a câmera \textit{Pi}. Os autores apresentam a implementação de metodologias e algoritmos com seu fluxo de ação para a captação de imagens sem ruídos. O conceito de processamento de imagens é explanado de maneira sucinta, como a captura por câmera, processamento através de um sistema e amostragem da imagem processada, conforme fluxos representados nas Figuras 7 e 8.

\begin{figure}[htb]
    \legenda[fig:fluxo_shilpashree]{Fluxo de execução proposto para processamento}
  \fig{scale=0.5}{img/fluxo_shilpashree}
    \fonte{\citeonline{shilpashree2015implementation}}
\end{figure}

\begin{figure}[htb]
    \legenda[fig:fluxo_simples_shilpashree]{Visão geral do fluxo}
  \fig{scale=0.75}{img/fluxo_simples_shilpashree}
    \fonte{\citeonline{shilpashree2015implementation}}
\end{figure}

O artigo apresenta o dispositivo \textit{Raspberry Pi}, uma placa com \textit{chip Broadcom} 700MHz, com CPU ARM 32 \textit{bits}, rodando um sistema operacional \textit{Linux} a partir de um cartão SD. Os autores utilizaram um módulo de câmera de 15 pinos, com resolução de 5MP, o que traz como resultado uma imagem de até 1920 x 1080 pixels e um vídeo com captura de aproximadamente 30 quadros por segundo. O módulo utiliza conectores \textit{Camera Serial Interface} (CSI) para serem inseridos diretamente na interface desenvolvida para câmeras do dispositivo, sendo sua lente capaz de entregar um vídeo de alta qualidade.

A metodologia aplicada traz a instalação do sistema operacional a ser utilizado na placa, bem como a inserção do módulo de câmera. Os algoritmos foram desenvolvidos usando a linguagem de programação \textit{Python}. O algoritmo utilizado para demonstrar o processamento de imagem realizado na placa \textit{Raspberry Pi} é \textit{Rudin-Osher-Fatemi (ROF)}. Esse algoritmo aplica uma suavização na imagem, retirando os ruídos da imagem captada, minimizando as diferenças para a imagem desejada e preservando as bordas e estruturas reais. Como resultado da aplicação, o algoritmo apresentou imagens mais nítidas, tendo sido executado com sucesso no dispositivo (Figura 9).

\begin{figure}[htb]
    \legenda[fig:suavizacao_shilpashree]{Suavização com ROF}
  \fig{scale=0.75}{img/suavizacao_shilpashree}
    \fonte{\citeonline{shilpashree2015implementation}}
\end{figure}


\section{integração do raspberry pi, arduino e opencv para processamento inteligente de imagens aplicados a um robô móvel teleoperado}
Gaier \textit{et al.} (2013) apresentam o processamento de imagens por meio da biblioteca OpenCV implementada no computador \textit{Raspberry Pi}, atrelado a um robô construído a partir dos componentes da plataforma Arduino (Figura 10) com o propósito da identificação de padrões de maneira inteligente.

\begin{figure}[htb]
    \legenda[fig:robo_gaier]{Robô construído para a aplicação}
  \fig{scale=0.75}{img/robo_gaier}
    \fonte{\citeonline{gaier2013}}
\end{figure}

Para a identificação foi utilizada a biblioteca OpenCV, que tem como seus diferenciais ser multiplataforma e focada em ambientes móveis, por ser rápida. O controle de movimentos e resposta do robô ao ambiente foram configurados dentro do Arduino Uno, plataforma específica para elaboração de projetos eletrônicos, com maior popularidade na robótica, onde sua configuração e instalação é fácil e prática, além das respostas serem ágeis, pela maneira como é instalada, diretamente nos circuitos do robô.
As técnicas utilizadas para a identificação de padrões incluem a detecção de bordas por meio do algoritmo de \textit{Canny}, que estabelece como premissas a não perda de nenhuma borda presente na imagem, a maior precisão nas bordas com relação ao mundo real e a completa eliminação de ruído, para que uma borda seja estabelecida apenas uma vez; e também o \textit{Speeded Up Robust Features} (SURF), técnica que traduz a imagem a um conjunto de coordenadas cartesianas para que seja criado um padrão de escala (Figura 11), a ser aplicado em outro cenário onde deverá ser marcado e exibido um objeto correspondente. Ambas as técnicas aplicam transformações gaussianas e escalas de cinza para obter maior precisão sem perda de informação.

\begin{figure}[htb]
    \legenda[fig:aplicacao_tecnica_gaier]{Exemplo da aplicação da técnica SURF}
  \fig{scale=0.75}{img/aplicacao_tecnica_gaier}
    \fonte{\citeonline{gaier2013}}
\end{figure}

A implementação foi testada no robô montado, passando por cada ponto de tratamento e passo dos algoritmos. Com o robô treinado a partir de uma imagem (Figura 12), obteve-se êxito na busca pelo contorno dentro de um ambiente com certo atraso na visualização das respostas, porém não afetando no seu resultado, e com comunicação estável entre as diferentes plataformas.

\begin{figure}[htb]
    \legenda[fig:treino_identificacao_gaier]{Treino para identificação de padrões}
  \fig{scale=0.75}{img/treino_identificacao_gaier}
    \fonte{\citeonline{gaier2013}}
\end{figure}


\section{embedded image capturing system using raspberry pi}
Senthilkumar \textit{et al.} (2014) apresentam as técnicas de obtenção de imagens a partir do computador \textit{Raspberry Pi} e as possíveis aplicações do abordado, como câmeras em automóveis e elevadores inteligentes. O artigo trata com detalhes cada componente do computador e como esses podem ser aproveitados para a obtenção de imagens a partir da câmera projetada especificamente para o computador \textit{(Raspberry Pi Camera Board).}
Para a aplicação é especificado o sistema embarcado utilizando uma placa \textit{Raspberry Pi}, um módulo de câmera MIPI CSI e monitores. O método utilizado para exemplificar o uso de sistemas embarcados é a aquisição de imagens pela câmera e o seu armazenamento em um dispositivo, segundo fluxograma apresentado na Figura 13.

\begin{figure}[htb]
    \legenda[fig:fluxograma_senthilkumar]{Fluxograma do processamento}
  \fig{scale=0.5}{img/fluxograma_senthilkumar}
    \fonte{\citeonline{senthilkumar2014embedded}}
\end{figure}

Após a aquisição das informações, cada imagem é analisada conforme a me-todologia de reconhecimento facial \textit{Eigenfaces}. Tal método define a região de uma face a partir de imagens previamente adquiridas, classifica como reconhecida ou desconhecida a imagem atualmente adquirida, se é ou não é uma face e estabelece relações entre as informações atuais e a base.
Os autores concluíram que o sistema proposto é menor e mais ágil do que as aplicações de PC. Também seus resultados foram satisfatórios para o ambiente imposto.

\section{video segmentation by tracking many figure-ground segments}
Li \textit{et al.} (2013) propõem uma segmentação de múltiplas imagens abordada por um algoritmo que trata a superfície adquirida da segmentação inicial dos frames do vídeo. Tal segmentação de vídeo é realizada a partir da sobreposição de faixas do vídeo que é reconstruída a partir de algoritmos não-supervisionados.
O artigo propõe resolver o problema de segmentação de vídeo não normalizada, separando todos os frames simultaneamente para gerar uma única imagem disposta quadro-a-quadro. Consequentemente, gerar um modelo global de trilha para buscar e validar em cada frame se o padrão é mantido, atualizando assim sempre os modelos dentro da base. A aplicação do método construído é exemplificada na Figura 14.

\begin{figure}[htb]
    \legenda[fig:aplicacao_li]{Aplicação da segmentação de superfície, seguida da segmentação de faixas e refinamento da localização}
  \fig{scale=1.0}{img/aplicacao_li}
    \fonte{\citeonline{li2013video}}
\end{figure}

Para cada quadro foi utilizado um algoritmo de separação por múltiplas imagens e estabelecida a configuração de aparência. Logo após a diferenciação dos quadros, foi aplicada simultaneamente a aprendizagem dos modelos para todos os quadros da imagem, uma abordagem gulosa para os conseguintes quadros, repetindo o algoritmo. Neste artigo foi demonstrado que a múltipla secção para segmentação é possível e sua adoção melhora a interpretação de sequências de vídeos.

\section{detection and location of people in video images using adaptive fusion of color and edge information}
Jabri \textit{et al.} (2000) apresentam um método de extração e segmentação de pessoas baseado na coloração predominante e na área de borda, focando na área mais recentemente modificada. Tendo como premissa que a parte a ser segmentada será uma pessoa e que, em um vídeo, ela será a parte mais recente a ser inserida na imagem (Figura 15), o método analisa o contorno e as bordas das regiões de acordo com a frequência e canais de cores definidas na imagem.

\begin{figure}[htb]
    \legenda[fig:imagem_inicial_jabri]{Imagem em que a segmentação será aplicada}
  \fig{scale=1.0}{img/imagem_inicial_jabri}
    \fonte{\citeonline{jabri2000detection}}
\end{figure}

O método é dividido em três partes para segmentar a região onde estará a pessoa a ser identificada:
\begin{enumerate}
    \item Separar e manter o fundo da imagem.
    \item Subtrair da imagem original o fundo.
    \item Selecionar a parte frontal da imagem.
\end{enumerate}

A separação é realizada através da medida de peso de cada pixel em duas imagens subsequentes, onde cada vez mais ao fundo, mais leve e estático o pixel será. A resultante do método é apresentada na Figura 16.

\begin{figure}[htb]
    \legenda[fig:fundo_jabri]{Segmentação do fundo da cena}
  \fig{scale=1.0}{img/fundo_jabri}
    \fonte{\citeonline{jabri2000detection}}
\end{figure}

A separação através de bordas (Figura 17) é calculada pela alteração dos canais de cor, utilizando o fator Sobel, o qual realça os contornos através da intensidade dos pixels e consegue definir bordas horizontais e verticais, bem como o sentido da opacidade ocorrida, quando há.

\begin{figure}[htb]
    \legenda[fig:bordas_jabri]{Separação da pessoa através das bordas}
  \fig{scale=0.75}{img/bordas_jabri}
    \fonte{\citeonline{jabri2000detection}}
\end{figure}

A imagem frontal é obtida através da diferença de escala de cinza atribuída na imagem retirada. Para que se extraia com maior definição, é aplicada a extração de contornos, conforme mostra a Figura 18.

\begin{figure}[htb]
    \legenda[fig:resultado_jabri]{Resultado da diferença entre o vídeo original e a separação da cena}
  \fig{scale=1.0}{img/resultado_jabri}
    \fonte{\citeonline{jabri2000detection}}
\end{figure}

Como conclusão, os autores verificaram que a extração de ruídos é favorecida com a aplicação de borda e cor, se adaptando bem a baixas frequências, podendo ser a porta de entrada para cenários mais específicos em realidade virtual, interação e reconhecimento de gestos.

\section{uma implementação de um sistema de contagem de pessoas baseado em vídeo processamento}
Almeida \textit{et al.} (2014) apresentam uma abordagem de segmentação, rastreamento e contagem de pessoas segundo estratégias de clusterização, separação e associação de blocos através do algoritmo \textit{k-means}. A utilização de câmeras posicionadas de modo zenital é escolhida por ser o modo de sensoriamento mais preciso e com maior detalhamento de informações, superando os sensores infravermelhos ou mecânicos.
As técnicas de Processamento Digital de Imagens escolhidas pelos autores para a abordagem são a segmentação do plano de fundo, detecção de objetos em movimento através da diferença de frequência de escala de cinza na imagem \textit{(template matching)} e a escolha do posicionamento zenital para a captura, pois, segundo os autores, define uma constante para o tamanho dos objetos, a melhor visão no cenário, mantém privacidade não levando em consideração rostos e não tem necessidade de calibração.
A abordagem segue o seguinte fluxo:
\begin{lista}
    \item Captura de vídeo;
    \item Remoção do plano de fundo;
    \item Segmentação por \textit{k-means};
    \item Rastreamento;
    \item Validação;
    \item Detecção;
    \item Contagem.
\end{lista}

Utilizando a técnica \textit{k-means} após a diferença entre os canais da imagem e definição do que faz parte do fundo do vídeo, é estimado o número de centróides, que correspondem ao número de pessoas na cena, bem como a área de cada centroide, correspondendo ao tamanho médio de uma pessoa, ilustrado na Figura 19.

\begin{figure}[htb]
    \legenda[fig:kmeans_almeida]{Resultado da diferença entre o vídeo original e a separação da cena}
  \fig{scale=1.0}{img/kmeans_almeida}
    \fonte{\citeonline{deimplementaccao}}
\end{figure}

Após a definição de cada \textit{cluster} (centróide), correspondente a uma pessoa, o algoritmo proposto calcula a menor distância Euclidiana entre os centróides pela diferença de \textit{frames} consecutivos, marcando-os como a mesma pessoa.
A relação de \textit{cluster} por \textit{frame} é armazenada dentro de uma matriz, onde a posição \textit{t} corresponderá a 1, caso o mesmo \textit{cluster} estiver no correspondente \textit{frame} $t+1$. A contagem de pessoas, passo final do algoritmo, se dá através da análise da matriz resultante onde, caso exista a alteração dos valores de 0 para 1, um contador é somado.
Como experimento, o algoritmo foi treinado em dois ambientes, um instável e um controlado, com a mesma duração e o mesmo número máximo de pessoas. Observou-se que a distância da câmera ao chão afeta consideravelmente a distância entre os \textit{clusters}, prejudicando a contagem. Sendo assim, foi necessária a utilização de métodos de correção: precisão e \textit{recall}. A precisão é dada pela razão entre o ponto verdadeiro positivo e a soma verdadeiro positivo e falso positivo. O \textit{recall} é dado pela razão entre o ponto verdadeiro positivo e a soma verdadeiro positivo e falso negativo. O F-Score, relação média ponderada entre precisão e \textit{recall}, resulta em um valor mais real da diferença entre os clusters.
De acordo com o Quadro 1, foi visto que, com o melhor ajuste de parâmetros e filtro eliminando ruídos, os resultados seriam mais precisos.

\begin{quadro}[htb]
    \legenda[quadro:resultados_almeida]{Comparativo dos resultados obtidos}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{} & \textbf{Método Original}\\ 
        \hline\hline
        Pessoas & 20\\ \hline
        TP  & 20\\ \hline
        FP+FN    & 0+1\\ \hline
        Precisão  & 1.00\\ \hline
        \textit{Recall}  & 0.95\\ \hline
        F-score & 0.97\\ \hline
        Questão & Alternativa\\ \hline
    \end{tabular}
    
    \fonte{\cite{deimplementaccao}}
\end{quadro}

\section{avaliação da amostragem temporal na detecção e no rastreamento de pessoas em vídeos de fundo dinâmico}
Siqueira e Machado (2015) apresentam uma minimização no número de quadros \textit{(frames)} a serem analisados para a detecção e rastreamento de pessoas em vídeo. Utilizando como método principal para a segmentação, o autor baseou-se no classificador \textit{Adaboost}, esquematizado na Figura 20. O classificador consiste na combinação linear de características identificadas por meio do método \textit{Mean Shift}, que define um objeto segundo suas medidas ou cor e delimitando-as segundo o Filtro de \textit{Kalman.}
Seguindo a classificação, é aplicada uma função do tipo \textit{Haar} (Figura 21), definida pela transformada de \textit{Haar}, que consiste na subtração da média dos valores da região mais escura pelos valores da região mais clara da imagem. A classificação dita forte se dá pela taxa de acerto nos quadros selecionados, ou seja, onde existe o objeto a ser rastreado.

\begin{figure}[htb]
    \legenda[fig:classificador_adaboost_siqueira]{Esquematização do classificador Adaboost}
  \fig{scale=0.75}{img/classificador_adaboost_siqueira}
    \fonte{\citeonline{siqueiraavaliaccao}}
\end{figure}

\begin{figure}[htb]
    \legenda[fig:caracteristicas_haar_siqueira]{Esquematização de características do tipo Haar}
  \fig{scale=1.0}{img/caracteristicas_haar_siqueira}
    \fonte{\citeonline{siqueiraavaliaccao}}
\end{figure}

Seguindo a estratégia de diminuição de quadros, o autor utilizou-se de um sensor de captura de 16 frames por segundo. Após a aplicação dos métodos descritos, percebe-se, conforme mostra a Figura 22, que a detecção não se baseia diretamente na quantidade de elementos analisados, mas sim nas características definidas para o objeto a ser detectado ou rastreado, podendo assim diminuir a quantidade de quadros coletados para análise.

\begin{figure}[htb]
    \legenda[fig:resultado_siqueira]{Resultado da aplicação dos métodos Adaboost e Haar}
  \fig{scale=1.0}{img/resultado_siqueira}
    \fonte{\citeonline{siqueiraavaliaccao}}
\end{figure}

O Quadro 2 apresenta um comparativo entre os trabalhos apresentados e suas abordagens.

\begin{quadro}[htb]
    \legenda[quadro:comparativo_trabalhos]{Comparativo dos trabalhos relaconados}
    \begin{tabular}{|c||c|c|p{4cm}|}
        \hline
        \textbf{Trabalho} & \textbf{\textit{Raspberry Pi}} & \textbf{PDI} & \textbf{Abordagem} \\ 
        \hline\hline
        Shilpashree \textit{et al.} (2015) & Sim    & Sim   & Algoritmo de redução de ruído RUF   \\ \hline
        Gaier et al. (2013)  & Sim    & Sim   & Integração Arduino, Raspberry Pi, OpenCV para reconhecimento de padrões    \\ \hline
        Senthilkumar \textit{et al.} (2014)   & Sim   & Sim  & Algoritmo Eigenfaces    \\ \hline
        Li \textit{et al.} (2013)  & Não    & Sim  & Algoritmo de modelo global para a segmentação de vídeo  baseado na sobreposição de \textit{frames}   \\ \hline
        Jabri \textit{et al.} (2000)  & Não    & Sim   & Método de segmentação utilizando a separação de planos do vídeo    \\ \hline
        Almeida \textit{et al.} (2014)  & Não    & Sim   & Detecção e contagem de pessoas com base no algoritmo \textit{k-means}    \\ \hline
        Siqueira e Machado (2015) & Não   & Sim  & Minimização dos \textit{frames} utilizados para o rastreamento em vídeo    \\ \hline
    \end{tabular}
    
    \fonte{O próprio autor, 2017}
\end{quadro}

% -----------------------------------------------------------------------
% -----------------------------------------------------------------------
\chapter{Metodologia}
\label{cap:metodologia}

Este capítulo apresenta os materiais e metodologias aplicados no desenvolvimento do sistema de detecção de pessoas em vídeo. Para a implementação do trabalho proposto, foi utilizada a placa \textit{Raspberry Pi}, 3ª geração do seu modelo B, com o sistema operacional \textit{Rasbian} em sua versão \textit{Pixel}, de setembro de 2016, onde é realizado o processamento do vídeo capturado pelo módulo de câmera sob a biblioteca OpenCV, em sua versão 3.3.0, os quais são detalhados nas seções seguintes.

\section{materiais utilizados}

Neste trabalho foi utilizado o \textit{Raspberry Pi} 3, modelo B com 1GB de memória e processador ARMv8 64-\textit{bit, quad-core} de 1.2GHz. O sistema operacional instalado no dispositivo é o \textit{Raspbian Jessie}, versão 8. Para a captura de imagens foi utilizado o módulo de câmera V2 com resolução de 8MP. Detalhes sobre o \textit{Raspberry Pi} e sua arquitetura são discutidos na Seção 2.3.

\subsection{OpenCV}
\textit{Open Source Computer Vision Library (OpenCV)} é uma biblioteca focada para desenvolvimento de aplicações voltadas ao Processamento Digital de Imagens. Desenvolvida em âmbito acadêmico pela Intel, em 1999, a biblioteca se expandiu para o campo comercial visando o crescimento da necessidade de sistemas focados em visão computacional. Tem como principais características:
\begin{lista}
	\begin{itemize}
	    \item Biblioteca multiplataforma, com implementação a diversas linguagens de programação, como por exemplo C++, Java, MatLab, Python, Perl e Ruby;
	\end{itemize}
	\begin{itemize}
	    \item Dividida em módulos: \textit{cv} (funções principais), \textit{highgui} (desenvolvimento de interface gráfica) e \textit{cxcore} (estruturas de dados e funções de álgebra linear);
	\end{itemize}
	\begin{itemize}
	    \item Focada no desenvolvimento de sistemas de Processamento Digital de Imagens, análise estrutural e de movimento, rastreamento, entre outros;
	\end{itemize}
	\begin{itemize}
	    \item Suas funções tratam de imagens após a etapa de pré-processamento, onde essas imagens podem ser redimensionadas, padronizadas e filtradas para diminuição de ruídos;
	\end{itemize}
	\begin{itemize}
	    \item As funções implementam: detecção mais detalhada de ruídos, extração de informações, dentre outras.
	\end{itemize}
\end{lista}

\section{metodologia}
Esta seção apresenta o processo utilizado para a segmentação de pessoas em vídeo, desenvolvido em linguagem de programação Python. A Figura 23 apresenta um fluxo detalhado do processo proposto. Os métodos e funções utilizados para o Processamento Digital de Imagens são detalhados na seção 2.1.5.

\begin{figure}[htb]
    \legenda[fig:diagrama_metodologia]{Fluxograma Desenvolvido para Segmentação Automática de Pessoas Utilizando Raspberry Pi}
  \fig{scale=0.75}{img/diagrama_metodologia}
    \fonte{O próprio autor, 2017}
\end{figure}
% -----------------------------------------------------------------------
% -----------------------------------------------------------------------
\chapter{Resultados Exeperimentais}
\label{cap:resultados}

Este capítulo apresenta os resultados obtidos a partir da metodologia construída na seção 4.2 aplicada à base de teste.

O local usado como base para teste dos algoritmos foi montado no corredor de laboratórios de informática da Universidade Tuiuti do Paraná, onde o dispositivo \textit{Raspberry Pi} foi acoplado a uma escada para simular a altura d um câmera de segurança.
Foram criados cenários que simulam a circulação de pessoas em um corredor, sendo eles:

\begin{lista}
    \item Uma pessoa se movimentando em direção à câmera.
    \item Uma pessoa se afastando da câmera.
    \item Duas pessoas no sentido da câmera.
    \item Duas pessoas em direção contrária à câmera.
    \item Três pessoas em direção à câmera.
    \item Três pessoas em direção contrária à câmera.
\end{lista}

As figuras exemplificam as imagens obtidas para teste.




O quadro 3 denota o resultado comparativo da aplicação das diferentes técnicas analisadas nos vídeos obtidos para a base de testes.

\begin{quadro}[htb]
    \legenda[quadro:resultados_boosting]{Resultados da Técnica de Rastreamento Boosting}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Vídeo} & \textbf{Pessoas em Cena} & \textbf{Pessoas Identificadas} \\ 
        \hline\hline
        1102438 & 1 & 1 \\ \hline
        1102439 & 1 & 1 \\ \hline
        1102635 & 1 & 1 \\ \hline
        1102723 & 1 & 1 \\ \hline
        1102833 & 2 & 1 \\ \hline
        1102944 & 2 & 2 \\ \hline
        1103014 & 2 & 2 \\ \hline
        1103213	& 3	& 3 \\ \hline
        1103332	& 3	& 2 \\ \hline
        1103339	& 3	& 3 \\ \hline
        1104130	& 1	& 1 \\ \hline
        1104233	& 1	& 1 \\ \hline
        1104355	& 3	& 3 \\ \hline
        1104450	& 3	& 1 \\ \hline
        1104515	& 3	& 2 \\ \hline
        1104627	& 2	& 2 \\ \hline
		1 pessoa_1 & 1 & 1 \\ \hline
        1_pessoa_1_1 &	1 &	1 \\ \hline
        1_pessoa_1_2 &	1 &	1 \\ \hline
        1_pessoa_1_3 &	1 &	1 \\ \hline
        1_pessoa_2 & 1 & 1 \\ \hline
        1_pessoa_3 & 1 & 1 \\ \hline
        1_pessoa_3_1 & 1 & 1 \\ \hline
        1_pessoa_3_2 & 1 & 1 \\ \hline
        1_pessoa_3_3 & 1 & 1 \\ \hline
        1_pessoa_4 & 1 & 1 \\ \hline
        1_pessoa_4_1 & 1 & 1 \\ \hline
        2_pessoas &	2 &	2 \\ \hline
        2_pessoas_2	& 2 & 2 \\ \hline
        2_pessoas_3	& 2 &2 \\ \hline
        2_pessoas_4	& 2 & 1 \\ \hline
        2_pessoas_5	& 2 & 2 \\ \hline
        2_pessoas_6	& 2	& 1 \\ \hline
        2_pessoas_7_ & 2 & 1 \\ \hline
        2_pessoas_8 & 2 &1 \\ \hline
        2_pessoas_9 & 2 & 1 \\ \hline
        3_pessoas_1 & 3 & 3\\ \hline
        3_pessoas_2 & 3 & 2 \\ \hline
        3_pessoas_3	& 3 & 2 \\ \hline
        3_pessoas_4 & 3 & 2 \\ \hline
        3_pessoas_5 & 3 & 1 \\ \hline
    \end{tabular}
    
    \fonte{O próprio autor, 2017}
\end{quadro}


\begin{quadro}[htb]
    \legenda[quadro:resultados_mil]{Resultados da Técnica de Rastreamento MIL}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Vídeo} & \textbf{Pessoas em Cena} & \textbf{Pessoas Identificadas} \\ 
        \hline\hline
        1102438	1	1
1102439	1	1
1102635	1	1
1102723	1	1
1102833	2	2
1102944	2	2
1103014	2	2
1103213	3	2
1103332	3	3
1103339	3	3
1104130	1	1
1104233	1	1
1104355	3	2
1104450	3	3
1104515	3	3
1104627	2	2
		
1_pessoa_1	1	1
1_pessoa_1_1	1	1
1_pessoa_1_2	1	1
1_pessoa_1_3	1	1
1_pessoa_2	1	1
1_pessoa_3	1	1
1_pessoa_3_1	1	1
1_pessoa_3_2	1	1
1_pessoa_3_3	1	1
1_pessoa_4	1	1
1_pessoa_4_1	1	1
2_pessoas	2	2
2_pessoas_2	2	2
2_pessoas_3	2	2
2_pessoas_4	2	2
2_pessoas_5	2	2
2_pessoas_6	2	2
2_pessoas_7	2	2
2_pessoas_8	2	2
2_pessoas_9	2	2
3_pessoas_1	3	3
3_pessoas_2	3	3
3_pessoas_3	3	3
3_pessoas_4	3	3
3_pessoas_5	3	3

    \end{tabular}
    
    \fonte{O próprio autor, 2017}
\end{quadro}


\begin{quadro}[htb]
    \legenda[quadro:resultados_kcf]{Resultados da Técnica de Rastreamento KCF}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Vídeo} & \textbf{Pessoas em Cena} & \textbf{Pessoas Identificadas} \\ 
        \hline\hline
        1102438	1	1
1102439	1	1
1102635	1	1
1102723	1	1
1102833	2	0
1102944	2	2
1103014	2	2
1103213	3	3
1103332	3	3
1103339	3	2
1104130	1	1
1104233	1	1
1104355	3	3
1104450	3	3
1104515	3	2
1104627	2	2
		
1_pessoa_1	1	1
1_pessoa_1_1	1	1
1_pessoa_1_2	1	1
1_pessoa_1_3	1	1
1_pessoa_2	1	1
1_pessoa_3	1	1
1_pessoa_3_1	1	1
1_pessoa_3_2	1	1
1_pessoa_3_3	1	1
1_pessoa_4	1	1
1_pessoa_4_1	1	1
2_pessoas	2	0
2_pessoas_2	2	2
2_pessoas_3	2	2
2_pessoas_4	2	2
2_pessoas_5	2	1
2_pessoas_6	2	2
2_pessoas_7	2	1
2_pessoas_8	2	2
2_pessoas_9	2	2
3_pessoas_1	3	0
3_pessoas_2	3	2
3_pessoas_3	3	3
3_pessoas_4	3	3
3_pessoas_5	3	2

    \end{tabular}
    
    \fonte{O próprio autor, 2017}
\end{quadro}


\begin{quadro}[htb]
    \legenda[quadro:resultados_tld]{Resultados da Técnica de Rastreamento TLD}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Vídeo} & \textbf{Pessoas em Cena} & \textbf{Pessoas Identificadas} \\ 
        \hline\hline
        1102438	1	1
1102439	1	1
1102635	1	1
1102723	1	1
1102833	2	0
1102944	2	2
1103014	2	2
1103213	3	3
1103332	3	3
1103339	3	2
1104130	1	1
1104233	1	1
1104355	3	3
1104450	3	3
1104515	3	2
1104627	2	2
		
1_pessoa_1	1	1
1_pessoa_1_1	1	1
1_pessoa_1_2	1	1
1_pessoa_1_3	1	1
1_pessoa_2	1	1
1_pessoa_3	1	1
1_pessoa_3_1	1	1
1_pessoa_3_2	1	1
1_pessoa_3_3	1	1
1_pessoa_4	1	1
1_pessoa_4_1	1	1
2_pessoas	2	0
2_pessoas_2	2	2
2_pessoas_3	2	2
2_pessoas_4	2	2
2_pessoas_5	2	1
2_pessoas_6	2	2
2_pessoas_7	2	1
2_pessoas_8	2	2
2_pessoas_9	2	2
3_pessoas_1	3	0
3_pessoas_2	3	2
3_pessoas_3	3	3
3_pessoas_4	3	3
3_pessoas_5	3	2

    \end{tabular}
    
    \fonte{O próprio autor, 2017}
\end{quadro}



\begin{quadro}[htb]
    \legenda[quadro:resultados_medianflow]{Resultados da Técnica de Rastreamento MedanFlow}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Vídeo} & \textbf{Pessoas em Cena} & \textbf{Pessoas Identificadas} \\ 
        \hline\hline
        1102438	1	1
1102439	1	1
1102635	1	1
1102723	1	1
1102833	2	1
1102944	2	2
1103014	2	2
1103213	3	2
1103332	3	2
1103339	3	3
1104130	1	0
1104233	1	1
1104355	3	3
1104450	3	2
1104515	3	2
1104627	2	2
		
1_pessoa_1	1	1
1_pessoa_1_1	1	1
1_pessoa_1_2	1	1
1_pessoa_1_3	1	1
1_pessoa_2	1	1
1_pessoa_3	1	1
1_pessoa_3_1	1	1
1_pessoa_3_2	1	1
1_pessoa_3_3	1	1
1_pessoa_4	1	1
1_pessoa_4_1	1	1
2_pessoas	2	2
2_pessoas_2	2	2
2_pessoas_3	2	2
2_pessoas_4	2	2
2_pessoas_5	2	1
2_pessoas_6	2	1
2_pessoas_7	2	2
2_pessoas_8	2	1
2_pessoas_9	2	1
3_pessoas_1	3	3
3_pessoas_2	3	1
3_pessoas_3	3	2
3_pessoas_4	3	2
3_pessoas_5	3	1

    \end{tabular}
    
    \fonte{O próprio autor, 2017}
\end{quadro}

% -----------------------------------------------------------------------
% -----------------------------------------------------------------------
%%\chapter{Cronograma}
%%\label{cap:cronograma}

%%Especifica o tempo que será dispensado para a pesquisa, planejado em cada uma 
%%das etapas. Sua estrutura pode ser apresentada em meses ou semanas, conforme os 
%%requisitos da instituição (tempo mínimo exigido e máximo permitido) e as etapas 
%%variam de acordo com as diferentes áreas do saber.

% -----------------------------------------------------------------------
% -----------------------------------------------------------------------
%%\chapter{Elementos de apoio}
%%\label{cap:apoio}

% -----------------------------------------------------------------------
%%\section{Ilustrações}
%%\label{cap:ilustracoes}

%%Ilustrações são elementos cuja função é complementar ao texto: são explicativas 
%%e informativas, não podendo apenas adornar ou enfeitar o trabalho. Fazem parte 
%%das ilustrações: desenhos, esquemas, fluxogramas, fotografias, gráficos, mapas, 
%%organogramas, plantas, quadros, retratos, figuras, imagens e outros.

%%A ilustração deve ser anunciada no texto – chamada pelo seu nmero (algarismos 
%%arábicos) – e inserida o mais próximo possível do trecho a que se refere.
%%Qualquer que seja o tipo de ilustração, sua identificação aparece na parte 
%%superior, precedida da palavra designativa (Figura, Mapa, Gráfico, etc.), 
%%seguida de seu número de ordem de ocorrência no texto, em algarismos arábicos, 
%%hífen ou travessão e do respectivo título.

%%Quando a ilustração for elaborada pelo(s) autor(es) do trabalho, deverá 
%%aparecer ``o próprio autor'' ou, no caso de trabalho em equipe, ``os próprios 
%%autores''. A \autoref{fig:grafico} apresenta um exemplo de gráfico. A 
%%\autoref{fig:figura} apresenta um exemplo de figura centralizada, enquanto a 
%%\autoref{fig:subfigura} apresenta exemplos de subfiguras.

%%\begin{grafico}[htb]
%%    \legenda[fig:grafico]{Exemplo de gráfico}
%%    \fig{scale=0.6}{imagens/teste}
%%    \fonte{\citeonline[p. 24]{araujo2012}}
%%\end{grafico}


%%\begin{figure}[htb]
%%    \legenda[fig:subfigura]{Exemplo de várias subfiguras}
%%    \sfig{scale=0.3}{imagens/teste}\hfil
%%    \sfig{scale=0.3}{imagens/teste}\hfil
%%    \sfig{scale=0.3}{imagens/teste}
    
%%    \lfig[s:a3]{scale=0.3}{imagens/teste}{zzzz}\hfil
%%    \lfig[s:a3]{scale=0.3}{imagens/teste}{zzzz}\hfil
%%    \lfig[s:a4]{scale=0.3}{imagens/teste}{yyyy}
    
%%    \fonte{teste}
%%\end{figure}

% -----------------------------------------------------------------------
%%\section{Tabelas e Quadros}
%%\label{sec:tabelas}

%%As tabelas não são consideradas ilustrações, mas, sim, elementos demonstrativos 
%%de síntese. Por serem autossuficientes, não complementam o texto, isto porque 
%%já fazem parte dele como uma organização estrutural esquematizada. Segundo o 
%%IBGE (1993), as tabelas apresentam dados e/ou informações oriundos de 
%%tratamento estatístico e sua inserção no decorrer dos trabalhos segue as mesmas 
%%regras aplicadas para as ilustrações (identificação na parte superior com 
%%número e título, e fonte de referência na parte inferior em letra menor).

%%As tabelas podem ser inseridas no texto ou em anexo (principalmente as de 
%%formato grande, que ocupam uma página inteira ou mais). Recomenda-se incluir a 
%%observação ``continua...'' e ``... continuação'' nas respectivas partes, quando 
%%a tabela ocupar mais de uma página. Quando inseridas no texto, devem ser 
%%alinhadas às margens laterais ou centralizadas, se apresentarem formato pequeno.

%%Em suas delimitações, são usados traços horizontais para destacar o cabeçalho, 
%%bem como traço horizontal final. Indica-se a delimitação, no alto e em baixo, 
%%por traços horizontais grossos, preferencialmente. Não deve ser delineada à 
%%direita e à esquerda, por traços verticais e é facultativo o emprego de traços 
%%verticais para separação das colunas no corpo da tabela.

%%\begin{table}[htb]
%%    \legenda[tab:exemplo]{Exemplo de tabela}
%%    \begin{tabular}{c|ccc}
%%        \hline\
%%        \textbf{Pessoa} & \textbf{Idade} & \textbf{Peso} & \textbf{Altura} \\ 
%%        \hline\hline
%%        Marcos & 26    & 68   & 178    \\ 
%%        Ivone  & 22    & 57   & 162    \\ 
%%        ...    & ...   & ...  & ...    \\ 
%%        Sueli  & 40    & 65   & 153    \\ \hline
%%    \end{tabular}
    
%%    \fonteautor
%%\end{table}
 
%%Quando houver informações ou dados numéricos que não necessitem de cálculos 
%%(por exemplo, características, propriedades, relações, etc.), poderão ser 
%%utilizados os quadros. Nestes, os traços contornam toda a tabela.

%%\begin{lista}
%%	\item novo
%%	\item asd
%%	\item asd
%%	\item asd
%%	\item asd
%%\end{lista}

%%\begin{quadro}[htb]
%%    \legenda[quadro:exemplo]{Exemplo de quadro}
%%    \begin{tabular}{|c||c|c|c|}
%%        \hline
%%        \textbf{Pessoa} & \textbf{Idade} & \textbf{Peso} & \textbf{Altura} \\ 
%%        \hline\hline
%%        Marcos & 26    & 68   & 178    \\ \hline
%%        Ivone  & 22    & 57   & 162    \\ \hline
%%        ...    & ...   & ...  & ...    \\ \hline
%%        Sueli  & 40    & 65   & 153    \\ \hline
%%    \end{tabular}
    
%%    \fonte{\cite{EIA649B}}
%%\end{quadro}

% -----------------------------------------------------------------------
%%\section{Equações}
%%\label{sec:equações}

%%Para facilitar a leitura, equações e fórmulas devem ser destacadas no texto e, 
%%se necessário, numeradas com algarismos arábicos entre parênteses, alinhados à 
%%direita. Na sequência normal do texto, é permitido o uso de uma entrelinha 
%%maior que comporte seus elementos (expoentes, índices, entre outros). A 
%%\autoref{eq:exemplo} apresenta um exemplo de equação.

%%\begin{equation}
%%\label{eq:exemplo}
%%C_{(A,B)} = \{ p \in
%%A\;|\;[(\overrightarrow{q_i-c}){\cdot}{\vec{n}}_c][(\overrightarrow{q_j-c}){\cdot}{\vec{n}}_c]
%%< 0 \}
%%\end{equation}

% -----------------------------------------------------------------------
% -----------------------------------------------------------------------
%%\section{Códigos e Algoritmos}
%%\label{sec:codigos}

%%Os códigos e algoritmos podem ser inseridos no texto usando comandos 
%%\texttt{codigo} e \texttt{algoritmo}, respectivamente. O \autoref{cod:fib} 
%%apresenta um exemplo de código em C.

%%\begin{codigo}[htb]
%%   \legenda[cod:fib]{Calcula Fibonacci}
%%    \begin{lstlisting}[language=C]
%%    int main() {
%%        int n, first = 0, second = 1, next, c;
        
%%        printf("Enter the number of terms\n");
%%        scanf("%d", &n);
        
%%        printf("First %d terms of Fibonacci series are :-\n", n);
        
%%        for (c = 0; c < n; c++){
%%            if (c <= 1) next = c;
%%            else {
%%                next = first + second;
%%                first = second;
%%                second = next;
%%            }
%%            printf("%d\n",next);
%%        }
        
%%        return 0;
%%    }
%%    \end{lstlisting}
    
%%    \fonteautor
%%\end{codigo}


% -----------------------------------------------------------------------
% -----------------------------------------------------------------------
%%\chapter{Conclusão}

%%É a parte final do trabalho, na qual se apresentam as conclusões 
%%correspondentes aos objetivos e às hipóteses: informa se os objetivos foram 
%%alcançados ou não – seguidos de justificativas e explicações caso os mesmos não 
%%tenham sido alcançados – bem como se as hipóteses foram negadas ou 
%%corroboradas. É possível que se apresentem também:

%%\begin{lista}
%%    \item comentários relativos aos resultados obtidos, fechando o raciocínio 
%%    por meio de um processo dedutivo,
    
%%    \item a importância dos resultados obtidos,
    
%%    \item a projeção da pesquisa, com estimativas para o uso dos resultados,
    
%%    \item a repercussão, informando quem será beneficiado e em quê,
    
%%    \item as limitações do trabalho, mostrando suas fragilidades ou
%%insuficiências,
%%    \item as dificuldades encontradas no decorrer da pesquisa, e
%%    \item indicações para trabalhos futuros, para a continuidade da
%%pesquisa pelo próprio autor e por outros.
%%\end{lista}

%%Veja \autoref{apendice:teste}.

% ----------------------------------------------------------
% ELEMENTOS PÓS-TEXTUAIS
% ----------------------------------------------------------
%\postextual
% ----------------------------------------------------------

% ----------------------------------------------------------
% Referências bibliográficas
% ----------------------------------------------------------
\bibliography{referencias}

% ----------------------------------------------------------
% Apêndices
% ----------------------------------------------------------
% Material complementar preparado pelo autor
%%\apendice[apendice:teste]{TESTE}

% ----------------------------------------------------------
% Anexos
% ----------------------------------------------------------
% Material complementar nao preparado pelo autor
%%\anexo[apendice:teste1]{TESTE}


\end{document}
